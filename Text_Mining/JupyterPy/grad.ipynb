{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "#-*-coding:utf-8-*-\n",
    "# Mecab installation needed\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=\"C:/mecab/mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kkma.sentences('이것은 형태소 분석기 입니다 아버지가방에들어가신다')\n",
    "#mecab.morphs('이것은 형태소 분석기 입니다 아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kkma.nouns('이것은 형태소 분석기 입니다 아버지가방에들어가신다')\n",
    "#mecab.nouns('이것은 형태소 분석기 입니다 아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kkma.pos('이것은 형태소 분석기 입니다 아버지가방에들어가신다')\n",
    "#mecab.pos('이것은 형태소 분석기 입니다 아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한글', '형태소', '분석기', '코모', '란', '테스트', '중', '이', 'ㅂ니다', '.', '^', '^', '오예', '!']\n",
      "['한글', '형태소', '분석기', '코모', '테스트', '중', '오예']\n",
      "[('한글', 'NNP'), ('형태소', 'NNP'), ('분석기', 'NNG'), ('코모', 'NNP'), ('란', 'JX'), ('테스트', 'NNP'), ('중', 'NNB'), ('^', 'SW'), ('^', 'SW'), ('오예', 'NNP'), ('!', 'SF')]\n",
      "\n",
      "\n",
      " ('한글', 'NNP')\n",
      "\n",
      "\n",
      " ('형태소', 'NNP')\n",
      "\n",
      "\n",
      " ('분석기', 'NNG')\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      " 한글\n",
      "\n",
      "\n",
      " NNP\n"
     ]
    }
   ],
   "source": [
    "#사용자 사전 추가 링크\n",
    "#https://datascienceschool.net/view-notebook/4bfa8007982d4c7ba35d8b42cecd38c9/\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "print(komoran.morphs(u'한글형태소분석기 코모란 테스트 중입니다. ^^ 오예 !'))\n",
    "print(komoran.nouns(u'한글형태소분석기 코모란 테스트 중입니다 ^^ 오예 !'))\n",
    "print(komoran.pos(u'한글형태소분석기 코모란 테스트 중 ^^ 오예 !'))\n",
    "\n",
    "arr_komoran = komoran.pos(u'한글형태소분석기 코모란 테스트 중 ^^ 오예 !')\n",
    "print(\"\\n\\n\", arr_komoran[0])\n",
    "print(\"\\n\\n\", arr_komoran[1])\n",
    "print(\"\\n\\n\", arr_komoran[2])\n",
    "\n",
    "print(type(arr_komoran))\n",
    "print(\"\\n\\n\", arr_komoran[0][0])\n",
    "print(\"\\n\\n\", arr_komoran[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지와 라이브러리를 가져옴\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('버전: ', mpl.__version__)\n",
    "# print ('설치 위치: ', mpl.__file__)\n",
    "# print ('설정 위치: ', mpl.get_configdir())\n",
    "# print ('캐시 위치: ', mpl.get_cachedir())\n",
    "# print ('설정파일 위치: ', mpl.matplotlib_fname())\n",
    "\n",
    "#font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "# ttf 폰트 전체갯수\n",
    "#print(\"\\n\\n시스템폰트 전체갯수:\", len(font_list)) \n",
    "\n",
    "# OSX 의 설치 된 폰트를 가져오는 함수\n",
    "#font_list_mac = fm.OSXInstalledFonts()\n",
    "#print(\"\\n설치된 폰트\", len(font_list_mac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"시스템폰트 리스트에서 상위 10개만 출력\")\n",
    "# for i in range(10) :\n",
    "#         print(i+1, \"\\t\", font_list[i], \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = [f.name for f in fm.fontManager.ttflist]\n",
    "\n",
    "# print(\"저장된 폰트중에서 10개만 출력\")\n",
    "# for i in range(10) :\n",
    "#         print(i+1, \"\\t\", f[i], \"\\t\")\n",
    "\n",
    "\n",
    "# [(f.name, f.fname) for f in fm.fontManager.ttflist\n",
    "#      if 'lower' in f.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json파일 인코딩 문제 해결 코드\n",
    "\n",
    "def file_convert(filename):\n",
    "    convert_filename = 'result444.json'\n",
    "    bytes = min(32, os.path.getsize(filename))\n",
    "    raw = open(filename, 'rb').read(bytes)\n",
    "    \n",
    "    if raw.startswith(codecs.BOM_UTF8):\n",
    "        encoding = 'utf-8-sig'\n",
    "        \n",
    "    else:\n",
    "        result = chardet.detect(raw)\n",
    "        encoding = result['encoding']\n",
    "        \n",
    "    infile = io.open(filename, 'r', encoding=encoding)\n",
    "    data = infile.read()\n",
    "    infile.close()\n",
    "    \n",
    "    oo = open(convert_filename, 'w', encoding='UTF8')\n",
    "    oo.write(data)\n",
    "    oo.close()\n",
    "    \n",
    "    return convert_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CODE 1] 빈도수 체크 그래프 그리기\n",
    "def showGraph(wordInfo):\n",
    "    \n",
    "    font_location = \"c:/Windows/fonts/SangSangFlowerRoad.ttf\"\n",
    "    font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "    matplotlib.rc('font', family=font_name)\n",
    "    \n",
    "    #font_location = \"c:/Windows/fonts/Josefin Sans.ttf\"\n",
    "    #font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "    #matplotlib.rc('font', family=font_name)\n",
    "\n",
    "    plt.xlabel('주요 단어')\n",
    "    plt.ylabel('빈도수')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    Sorted_Dict_Values = sorted(wordInfo.values(), reverse=True)\n",
    "    Sorted_Dict_Keys = sorted(wordInfo, key=wordInfo.get, reverse=True)\n",
    "\n",
    "    plt.bar(range(len(wordInfo)), Sorted_Dict_Values, align='center')\n",
    "    plt.xticks(range(len(wordInfo)), list(Sorted_Dict_Keys), rotation='70')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CODE 2] 새 창에 워드 클라우드 생성\n",
    "def saveWordCloud(wordInfo, filename):\n",
    "    \n",
    "    taglist = pytagcloud.make_tags(dict(wordInfo).items(), maxsize=80)\n",
    "    pytagcloud.create_tag_image(taglist, filename, size=(640, 480), fontname=\"korean\", rectangular=False)\n",
    "    webbrowser.open(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 파일 읽기\n",
    "#-*- encoding: utf8 -*-\n",
    "import sys\n",
    "from functools import update_wrapper\n",
    "from django.conf import settings\n",
    "from django.core.exceptions import ImproperlyConfigured\n",
    "from django.db.models.base import ModelBase\n",
    "from django.views.decorators.cache import never_cache\n",
    "from imp import reload\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "# 명사 추출 및 빈도 분석\n",
    "import json\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.utils import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib as mpl\n",
    "\n",
    "import pytagcloud\n",
    "import webbrowser\n",
    "\n",
    "\n",
    "#from __future__ import print_function\n",
    "from lexrankr import LexRank\n",
    "from lexrank import STOPWORDS\n",
    "\n",
    "# 그래프를 노트북 안에 그리기 위해 설정\n",
    "%matplotlib inline\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import chardet\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import MeCab\n",
    "\n",
    "from threading import Thread\n",
    "import jpype\n",
    "\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# 코드 6-3 케라스를 사용한 단어 수준의 원-핫 인코딩하기\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#1차 정제\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from soynlp.noun import LRNounExtractor\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "#from soynlp.noun import LRNounExtractor_2\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from soynlp.tokenizer import RegexTokenizer\n",
    "from soynlp.postagger import Dictionary\n",
    "from soynlp.postagger import LRTemplateMatcher\n",
    "from soynlp.postagger import LREvaluator\n",
    "from soynlp.postagger import SimpleTagger\n",
    "from soynlp.postagger import UnknowLRPostprocessor\n",
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study():\n",
    "    #여기서 파일의 경로는 실제 JSON 데이터가 저장된 경로이다\n",
    "    openFileName = 'C:/Users/DH\\Desktop/JupyterPy/rawdata/crawling/result1.json'\n",
    "    #openFileName = file_convert(\"result444.json\")\n",
    "    \n",
    "    cloudImagePath = openFileName + '.jpg'\n",
    "    rfile = open(openFileName, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    jsonData = json.loads(rfile)\n",
    "    contents = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #[CODE 3]\n",
    "    \n",
    "    for item in jsonData:\n",
    "        if 'contents' in item.keys():\n",
    "            print(\"\\n[content]\\n\", content, \"\\n\\n\")\n",
    "            #content = content + re.sub(r'[^\\w]', ' ', item['content']) + ' '\n",
    "            content = content + re.sub('\\n\\n', '.\\n', item['content']) + ' '\n",
    "        \n",
    "        \n",
    "    #[CODE 4]\n",
    "    kkma = Kkma()\n",
    "    nouns = kkma.nouns(content)\n",
    "    count = Counter(nouns)\n",
    "    \n",
    "    \n",
    "    #[CODE 6]\n",
    "    print(\"\\n[원문]\\n\", content, \"\\n\\n\")\n",
    "    lexrank = LexRank()\n",
    "    lexrank.summarize(content)\n",
    "\n",
    "    summaries = lexrank.probe()\n",
    "    print(\"\\n[요약문 전체]\",\"\\n\", summaries, \"\\n\")\n",
    "\n",
    "    for i, summary in enumerate(summaries):\n",
    "        print(\"\\n[요약문 일부\", str(i), \"]\\t\", summary, \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #[CODE 5]\n",
    "    wordInfo = dict()\n",
    "    for tags, counts in count.most_common(50):\n",
    "        if (len(str(tags)) > 1):\n",
    "            wordInfo[tags] = counts\n",
    "            print (\"%s : %d\" % (tags, counts))\n",
    "            \n",
    "    plt.style.use('seaborn-pastel')      \n",
    "    \n",
    "    #showGraph(wordInfo) #그래프\n",
    "    #saveWordCloud(wordInfo, cloudImagePath) #워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쓰레드 예시 (나중에 쓸거같아서 일단 작성)\n",
    "def sentencing_with_multithread(lines):\n",
    "    nlines = len(liens)\n",
    "    results = []\n",
    "    t1 = Thread(target=sentencing, arg=(0, int(nlines/2), lines, results))\n",
    "    t2 = Thread(target=sentencing, arg=(int(nlines/2), nlines, lines, results))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    jpype.detachThreadFromJVM()\n",
    "    return sum(sum(results, []), [])\n",
    "\n",
    "\n",
    "def sentencing(start, end, lines, results):\n",
    "    jpype.attachThreadToJVM()\n",
    "    sentences = [kkma.sentences(lines[i]) for i in range(start, end)]\n",
    "    results.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, cprev):\n",
    "    xs, hs, cs, is_, fs, os, gs, ys, ps= {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev) # t=0일때 t-1 시점의 hidden state가 필요하므로\n",
    "    cs[-1] = np.copy(cprev)\n",
    "    loss = 0\n",
    "    #H = hidden_size\n",
    "    H = 100\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((len(inputs), 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        tmp = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh  # hidden state\n",
    "        is_[t] = sigmoid(tmp[:H])\n",
    "        fs[t] = sigmoid(tmp[H:2 * H])\n",
    "        os[t] = sigmoid(tmp[2 * H: 3 * H])\n",
    "        gs[t] = np.tanh(tmp[3 * H:])\n",
    "        cs[t] = fs[t] * cs[t-1] + is_[t] * gs[t]\n",
    "        hs[t] = os[t] * np.tanh(cs[t])\n",
    "\n",
    "    # compute loss\n",
    "    for i in range(len(targets)):\n",
    "        idx = len(inputs) - len(targets) + i\n",
    "        ys[idx] = np.dot(Why, hs[idx]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[idx] = np.exp(ys[idx]) / np.sum(np.exp(ys[idx]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[idx][targets[i], 0])  # softmax (cross-entropy loss)\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext, dcnext = np.zeros_like(hs[0]), np.zeros_like(cs[0])\n",
    "    n = 1\n",
    "    a = len(targets) - 1\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        if n > len(targets):\n",
    "            continue\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[a]] -= 1  # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dc = dcnext + (1 - np.tanh(cs[t]) * np.tanh(cs[t])) * dh * os[t]  # backprop through tanh nonlinearity\n",
    "        dcnext = dc * fs[t]\n",
    "        di = dc * gs[t]\n",
    "        df = dc * cs[t-1]\n",
    "        do = dh * np.tanh(cs[t])\n",
    "        dg = dc * is_[t]\n",
    "        ddi = (1 - is_[t]) * is_[t] * di\n",
    "        ddf = (1 - fs[t]) * fs[t] * df\n",
    "        ddo = (1 - os[t]) * os[t] * do\n",
    "        ddg = (1 - gs[t]^2) * dg\n",
    "        da = np.hstack((ddi.ravel(),ddf.ravel(),ddo.ravel(),ddg.ravel()))\n",
    "        dWxh += np.dot(da[:,np.newaxis],xs[t].T)\n",
    "        dWhh += np.dot(da[:,np.newaxis],hs[t-1].T)\n",
    "        dbh += da[:, np.newaxis]\n",
    "        dhnext = np.dot(Whh.T, da[:, np.newaxis])\n",
    "        n += 1\n",
    "        a -= 1\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1], cs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_lexranked(contents):\n",
    "    \n",
    "    #데이터 요약\n",
    "#     if (contents):\n",
    "#         return \"\"\n",
    "    try:\n",
    "        contents = re.sub('\\n\\n', '.\\n', contents)\n",
    "        lexrank = LexRank()\n",
    "        lexrank.summarize(contents)\n",
    "        summaries = lexrank.probe()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "#     for i, summary in enumerate(summaries):\n",
    "#         print(str(i), summary)\n",
    "    reform_list = summaries\n",
    "    \n",
    "    return reform_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(sentence):\n",
    "    # 1차 정제\n",
    "    #data = \"<p>\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n\\n 2019년 7월 20일 새벽 6시 9분ㅋㅋㅋㅏㅏㅏㅏㅣㅣ.<br/></p>\"\n",
    "    soup = BeautifulSoup(sentence, \"html5lib\")\n",
    "    remove_tag = soup.get_text()\n",
    "    result_text = re.sub('[-_,=+#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》;\\n,[ㄱ-ㅎ,ㅏ-ㅣ]', ''\n",
    "                         , remove_tag)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleasing2(text):\n",
    "    text = cleansing(text)\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail주소제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s]' # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_csv(csv_file_path):\n",
    "    csv_file = csv_file_path + \"/output.csv\"\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file_writer:\n",
    "        with open(csv_file, 'r', newline='', encoding='utf-8') as file_reader:\n",
    "            for row in file_reader:\n",
    "                print(row + \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(csv_file_path, vec):\n",
    "    \n",
    " # 기존 파일 있다면\n",
    "    try:\n",
    "        csv_file = str(glob.glob(csv_file_path + \"/output.csv\")[0])\n",
    "    \n",
    "        with open(csv_file, 'a', newline='', encoding='utf-8') as file_writer:\n",
    "            with open(csv_file, 'r', newline='', encoding='utf-8') as file_reader:\n",
    "                writer = csv.writer(file_writer)\n",
    "                writer.writerow(vec)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    # 새로 만들어야 한다면\n",
    "    except:    \n",
    "        csv_file = csv_file_path + \"/output.csv\"\n",
    "        fields=['제목','요약문', '키워드', 'score']\n",
    "        \n",
    "        with open(csv_file, 'wt', newline='', encoding='utf-8') as file_writer:\n",
    "            with open(csv_file, 'r', newline='', encoding='utf-8') as file_reader:\n",
    "                writer = csv.writer(file_writer)\n",
    "                writer.writerow(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(reform_list):\n",
    "    spliter = Kkma()\n",
    "    keyword_list = list()\n",
    "    #temp_text = cleasing2(' '.join(reform_list))  # 요약문에서 명사만 추출\n",
    "    temp_text = cleasing2(reform_list)  # 원문에서 명사만 추출\n",
    "    \n",
    "    if len(temp_text) > 0:\n",
    "        pos = spliter.pos(temp_text)\n",
    "        for keyword, type in pos:\n",
    "            if type == \"NNG\" or type == \"NNP\":\n",
    "                keyword_list.append(keyword)\n",
    "        \n",
    "    #keyword_list = spliter.nouns(' '.join(reform_list)) \n",
    "    keyword_count = Counter(keyword_list)\n",
    "    #print(keyword_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    keyword_list_upper = list()\n",
    "    for n, c in keyword_count.most_common(5): # 상위 5개만\n",
    "        #temp = {'명사': n, '빈도수': c}\n",
    "        temp = {n,c}\n",
    "        #if(len(n) >= 2):\n",
    "            #keyword_count.append(temp)\n",
    "            #keyword_list_upper.append(temp)\n",
    "        keyword_list_upper.append(n)\n",
    "\n",
    "    #keyword.append(temp['n'])\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    \n",
    "    print(keyword_list_upper, \"\\n\\n\")\n",
    "    return keyword_list_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rawdata(path_dir_json, json_files):\n",
    "    \n",
    "    # 빈 데이터프레임 정의\n",
    "    jsons_dataframe = pd.DataFrame(columns=['title', 'href', 'contents', 'reform_list'])\n",
    "    #jsons_dataframe.set_index('index_id', inplace=True)\n",
    "    #display(pd.DataFrame(jsons_dataframe))\n",
    "    \n",
    "    # 모든 json 하나씩 read 하기\n",
    "    for index, js in enumerate(json_files):  \n",
    "        with open(os.path.join(path_dir_json, js), encoding='utf-8-sig') as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            \n",
    "            \n",
    "            # 태그별 데이터 저장\n",
    "            title = cleasing2(json_text['title'])\n",
    "            href = json_text['href']\n",
    "            contents = cleasing2(json_text['contents'])\n",
    "            reform_list = sent_lexranked(contents)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 요약문에 대한 핵심 키워드 추출\n",
    "            keyword = find_keyword(contents)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # csv 파일 검색 후 append\n",
    "            vec = [title, reform_list, keyword, 0]\n",
    "            csv_file_path = \"C:/Users/DH/Desktop/wherewego-master/Source/csvdir\"\n",
    "            append_to_csv(csv_file_path, vec)\n",
    "            \n",
    "            \n",
    "    #print(vec)\n",
    "    read_to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_files(path_dir_json):\n",
    "    ##### 01 디렉토리 파일에서 json 파일들 찾기\n",
    "    json_files = [pos_json for pos_json in os.listdir(path_dir_json)\n",
    "                      if pos_json.endswith('.json')]\n",
    "    return json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ##### 00 디렉토리 파일 경로설정\n",
    "    path_dir_json = u'C:/Users/DH/Desktop/wherewego-master/Source/resultdir/소량테스트'\n",
    "    #path_dir_json = u'C:/Users/DH/Desktop/wherewego-master/Source/resultdir/서울 근교 힐링'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### 01 경로에서에서 json 파일들 찾기\n",
    "    json_files = find_json_files(path_dir_json)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### 02 모든 json 하나씩 read 하기\n",
    "    read_rawdata(path_dir_json, json_files)\n",
    "    \n",
    "    \n",
    "    #print(df.iloc[:, [0,1,3]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DH\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['곳', '생각', '서울', '전시', '정원'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DH\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['서울', '곳', '생각', '사진', '사람'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DH\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['광장', '시장', '육회', '동대문', '서울'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DH\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딸', '서울', '여행', '북촌', '진이'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DH\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['서울', '사진', '숲', '여행', '곳'] \n",
      "\n",
      "\n",
      "제목,요약문,키워드,score\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "서울 여행 북악스카이웨이 팔각정 드라이브 코스 돌고 왔어요,\"['오늘 이른 새벽 6시 25분 버스를 타서 서울로 올라오기 위해 밤을 새우기도 했고 심지어 며칠 내내 잠을 제대로 못 자저를 보는 사람들마다 다들 피곤해 보인다는 얘기를 하는 요즘 하루하루 정말 힘들기도 하고 너무 졸리지만가족들과 시간을 보내기 위해 참고 견디고 있는 중이랍니다 그리고 방금 전에는 서울 드라이브코스 라고 잘 알려진북악스카이웨이 팔각정 다녀왔는데요 TV에서도 봤던 곳이라 기대를 크게 하고 갔는데 결론만 얘기하지만 기대 이하였다는 사실볼거리가 좀 많을 줄 알았고 경치가 뛰어날 거라 생각했는데 일단 야경도 부족하고 그 위에서 즐길 것도 부족했답니다그렇기 때문에 여러분들에게 강력하게 추천해준다는 얘기는 할 수 없겠지만 그래도 관심 있는 분들도 계실 것 같아서어떤 곳이었는지 한 번 보여드리도록 하겠습니다', '요즘 저희 엄마도 추억을 기록하는 사진을 소중히 생각하셔서 제가 찍어준다고 하기도 전에먼저 사진 좀 찍어달라고 얘기하시곤 하신답니다하긴 언제 또 이렇게 서울 드라이브코스 찾아와서 함께 시간을 보내고  추억을 남기겠어요이곳 자체가 내가 생각하는 모습이 아니었다고 하더라도 이 순간을 함께 보낼 수 있는 소중한 가족이 있다는 것만으로도오늘 하루는 더없이 완벽한 하루를 보냈다고 얘기할 수 있을 것 같습니다']\",\"['서울', '곳', '생각', '사진', '사람']\",0\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "서울여행의 꽃 광장시장에서 먹방투어 대잔치  거리의 셰프들,\"['육회비빔이랑 파전 먹을까 하다가 여기서 왠 파전이냐 싶어서  그냥 일반 육회비빔밥 70 2개 시켰습니당 특이랑 뭐가 차이있냐고 여쭤보니 육회양이 더 많다고 하시길래 우린 앞으로 먹을게 많기에 일반으로 고고', '창신육회비빔밥 등장 처음에 보고서 얼레 육회 어딨어 밥은 왜이리 조금이야 라고 생각했으나 먹고 나니 딱 적당했다고']\",\"['광장', '시장', '육회', '동대문', '서울']\",0\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "딸과 단둘이서 떠난 서울여행,\"['아빠랑 아들의 점심 도시락을 준비하면서 저랑 딸이랑 기차에서 먹을 김밥도 함께 준비했어요 간식 등등 챙겨서 천천히 다녀면서 먹으려고 했으나 기차에서 싹 다 먹어버림요초딩 고학년이 되니 확실히 잘 먹어요설렘과 두려움을 안고 떠난다는 우리 딸또 너무너무 기대된다구요한국사를 참 좋아하는 아이라 궁궐 투어와 전쟁기념관을 일정으로 짰어요 서울여행 떠난다며 동전도 참 열심히 모았구요 참고로 동전은 9만 원이었어요  여러 분들께서 주신 용돈도 모아서 단단히 준비를 했기에 더 들떴지 싶어요', 'KTX 타고 서울역 내리자마자 도착했던 용산 전쟁기념관이에요공원이 너무 아름답고 예뻤던 곳이기도 했는데요 둘러보는 내내 마음이 착잡했고요 이거 실화냐며 지금 시대에서 살고 있는 것 자체가 큰 행운인 듯 미안하고 감사한 마음으로 둘러보았어요 하진이랑 눈물과 감동의 장소로 기억될 거 같습니다 전쟁은 되풀이되지 말아야 하겠어요1박2일의 시간 동안 좀 많이 구경하고 싶었던 서울여행이라 서울역 근교 관광지로 계획을 짰는데요 교통이 너무 편리했구요 대구 살면서 느껴보지 못한 마을버스도 타보구요 한옥체험 게스트하우스에서 머무르며 딸과 단둘이서 예쁜 추억을 만들었어요용돈을 모아 온 서울여행에서 우리하진이가요 다른건 몰라도 엄마 커피는 자기가 사 드리겠다는거예요 아이스아메리카노 주문한 후 저렇게 뿌듯한 표정으로 계산을 하네요 당이 떨어진 건 아닌데 기차에서 준비해 간 간식들 싹 다 먹고 딱 하나 남은 자유시간을 둘이 나눠도 먹었네요 많이 걸었더니 발바닥이 아파서 카페서 잠시 쉬는 것도 너무 달달한 시간이었어요']\",\"['딸', '서울', '여행', '북촌', '진이']\",0\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "서울여행 세번째로 서울숲으로 완전짱,\"['여기가 또 포토존이라고 합니다사진이 정말 잘나오는 자리에요멀리서 찍으면 더 잘나와요꼭 한번 가서 찍어보세요정말 잘나오는 자리에요 히히서울여행 세번째로 서울숲으로 완전짱', '위에 찍다가 카메라가 돌아가서찍혀버렸는데 의외로 잘나와서함 올려봤습니다위에 풍경을 찍다가 얼굴이 찍혀서 깜짝 놀랐거든요역시 전면카메라가 화질이 안좋아서 제가 잘나왔네요카메라는 역시 화질이 안좋아야 좋네요 히히']\",\"['서울', '사진', '숲', '여행', '곳']\",0\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "끝\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#임포트말고 직접실행\n",
    "if __name__ == \"__main__\":\n",
    "    #study()\n",
    "    #print(\"===============================================================================================\")\n",
    "    main()\n",
    "    print(\"\\n\\n\\n끝\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
